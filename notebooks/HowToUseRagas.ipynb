{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19011a0f-f7c0-4cd1-80e2-9dac0ccaaf66",
   "metadata": {},
   "source": [
    "### Setup RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b454567-7ea1-4ab4-82ce-e0365f8fdf7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import faiss\n",
    "import openai\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02ce22c9-b8c0-49ea-a9bb-89a5ca4ffca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"Paris is the capital and most populous city of France. The city is famed for the Eiffel Tower.\",\n",
    "    \"Jane Austen was an English novelist best known for 'Pride and Prejudice' and 'Sense and Sensibility'.\",\n",
    "    \"The Great Wall of China is a series of fortifications built to protect the ancient Chinese states.\",\n",
    "    \"Mount Everest, part of the Himalayas, is Earth’s highest mountain above sea level.\",\n",
    "    \"Mike loves the color pink more than any other color.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4e38813-6dbe-413d-84ed-72c70fb21059",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79fc2744-9dcd-445e-a43f-15a4fedc672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    response = client.embeddings.create(model=\"text-embedding-3-small\", input=text)\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d46fb890-9b3e-42ee-976e-9eccb0581137",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array([get_embedding(d) for d in docs]).astype('float32')\n",
    "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "faiss.normalize_L2(embeddings)\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa897de3-d6e7-456f-bfaf-5c97d07644fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, k):\n",
    "    query_embedding = np.array([get_embedding(query)]).astype(\"float32\")\n",
    "    \n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    _, idx = index.search(query_embedding, k)\n",
    "    \n",
    "    return [docs[i] for i in idx[0]]\n",
    "    \n",
    "\n",
    "def generate_answer(question, contexts):\n",
    "    prompt = (\n",
    "        \"Answer the user question **only** with facts found in the context.\\n\\n\"\n",
    "        \"Context:\\n\"\n",
    "        + \"\\n\".join(f\"- {c}\" for c in contexts)\n",
    "        + f\"\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c13255-3d7f-4dc1-a89b-7348ac9605d5",
   "metadata": {},
   "source": [
    "### Evaluate RAG System with Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74a5967f-bc31-4c39-bd26-c0377ce01522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Who wrote Pride and Prejudice?\",\n",
    "    \"Where is Mount Everest located?\",\n",
    "    \"What is Mike's favorite color?\"\n",
    "]\n",
    "\n",
    "ground_truths = [\n",
    "    \"Paris\",\n",
    "    \"Jane Austen\",\n",
    "    \"the Himalayas\",\n",
    "    \"Pink\"\n",
    "]\n",
    "\n",
    "rows = []\n",
    "\n",
    "for question, ground_truth in zip(questions, ground_truths):\n",
    "    context = retrieve(question, k=2)\n",
    "    answer = generate_answer(question, context)\n",
    "    rows.append(\n",
    "        {\n",
    "            \"question\": question,\n",
    "            \"contexts\": context,\n",
    "            \"answer\": answer,\n",
    "            \"reference\": ground_truth,\n",
    "        }\n",
    "    )\n",
    "\n",
    "evaluation_dataset = Dataset.from_list(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23dc0639-b149-4b1e-ad8b-313066726aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ca4635ad1e4f028204e1dd3932a110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question': 'What is the capital of France?', 'contexts': ['Paris is the capital and most populous city of France. The city is famed for the Eiffel Tower.', 'Mike loves the color pink more than any other color.'], 'answer': 'Paris is the capital of France.', 'reference': 'Paris'}, {'question': 'Who wrote Pride and Prejudice?', 'contexts': [\"Jane Austen was an English novelist best known for 'Pride and Prejudice' and 'Sense and Sensibility'.\", 'Mike loves the color pink more than any other color.'], 'answer': \"Jane Austen wrote 'Pride and Prejudice'.\", 'reference': 'Jane Austen'}, {'question': 'Where is Mount Everest located?', 'contexts': ['Mount Everest, part of the Himalayas, is Earth’s highest mountain above sea level.', 'Paris is the capital and most populous city of France. The city is famed for the Eiffel Tower.'], 'answer': 'Mount Everest is located in the Himalayas.', 'reference': 'the Himalayas'}, {'question': \"What is Mike's favorite color?\", 'contexts': ['Mike loves the color pink more than any other color.', 'Paris is the capital and most populous city of France. The city is famed for the Eiffel Tower.'], 'answer': \"Mike's favorite color is pink.\", 'reference': 'Pink'}]\n",
      "{'answer_correctness': 0.9719, 'answer_relevancy': 0.9985, 'faithfulness': 1.0000, 'context_precision': 1.0000, 'context_recall': 1.0000}\n"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_correctness,\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "\n",
    "scores = evaluate(\n",
    "    evaluation_dataset,\n",
    "    metrics=[\n",
    "        answer_correctness,\n",
    "        answer_relevancy,\n",
    "        faithfulness,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(rows)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74891303-632c-4326-bc09-17e0e82134d1",
   "metadata": {},
   "source": [
    "### Metrics Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f60920b-3423-492d-95b4-e800a60e74a4",
   "metadata": {},
   "source": [
    "https://docs.ragas.io/en/v0.1.21/concepts/metrics/answer_correctness.html\n",
    "\n",
    "https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/faithfulness/#example\n",
    "\n",
    "https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/context_precision/\n",
    "\n",
    "https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/context_recall/\n",
    "\n",
    "https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/answer_relevance/\n",
    "\n",
    "https://docs.ragas.io/en/v0.1.21/concepts/metrics/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5737a0-3d9d-4366-9b6f-b60df84e4422",
   "metadata": {},
   "source": [
    "### High Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48023cdd-3ed5-4da8-b48e-9754cbcb46c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72727e86574b492298f684db459dc6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'user_input': \"What is Mike's favorite color?\", 'retrieved_contexts': ['Mike loves the color pink more than any other color.'], 'response': \"Mike's favorite color is pink.\", 'reference': 'Pink'}]\n",
      "{'answer_correctness': 0.9645, 'answer_relevancy': 1.0000, 'faithfulness': 1.0000, 'context_precision': 1.0000, 'context_recall': 1.0000}\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "context = docs[-1:]\n",
    "question = questions[-1]\n",
    "answer = generate_answer(question, context)\n",
    "\n",
    "rows.append(\n",
    "    {\n",
    "        \"user_input\": question,\n",
    "        \"retrieved_contexts\": context,\n",
    "        \"response\": answer,\n",
    "        \"reference\": ground_truths[-1]\n",
    "    }\n",
    ")\n",
    "\n",
    "evaluation_dataset = Dataset.from_list(rows)\n",
    "\n",
    "scores = evaluate(\n",
    "    evaluation_dataset,\n",
    "    metrics=[\n",
    "        answer_correctness,\n",
    "        answer_relevancy,\n",
    "        faithfulness,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(rows)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d8a3ce-bf12-4f97-9ae7-ccde28b48ae6",
   "metadata": {},
   "source": [
    "### Wrong Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2ec7432-3d77-4b34-a4aa-5a1855157b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af090000b1a340c9bd65bfd41e5e2ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'user_input': \"What is Mike's favorite color?\", 'retrieved_contexts': ['Vienna is the capital of Austria'], 'response': \"The context does not provide information about Mike's favorite color.\", 'reference': 'Pink'}]\n",
      "{'answer_correctness': 0.1968, 'answer_relevancy': 0.0000, 'faithfulness': 1.0000, 'context_precision': 0.0000, 'context_recall': 0.0000}\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "context = ['Vienna is the capital of Austria']\n",
    "question = questions[-1]\n",
    "answer = generate_answer(question, context)\n",
    "\n",
    "rows.append(\n",
    "    {\n",
    "        \"user_input\": question,\n",
    "        \"retrieved_contexts\": context,\n",
    "        \"response\": answer,\n",
    "        \"reference\": ground_truths[-1]\n",
    "    }\n",
    ")\n",
    "\n",
    "evaluation_dataset = Dataset.from_list(rows)\n",
    "\n",
    "scores = evaluate(\n",
    "    evaluation_dataset,\n",
    "    metrics=[\n",
    "        answer_correctness,\n",
    "        answer_relevancy,\n",
    "        faithfulness,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(rows)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb52a4f-9ba1-4419-b1b5-18f674cece08",
   "metadata": {},
   "source": [
    "### Correct Answer with Wrong Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c326d32a-08dc-4d55-9a18-3b8a41dc238e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266a609152c345689ae0e1e1525daa5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'user_input': \"What is Mike's favorite color?\", 'retrieved_contexts': ['Vienna is the capital of Austria'], 'response': \"Mike's favorite color is pink!\", 'reference': 'Pink'}]\n",
      "{'answer_correctness': 0.9644, 'answer_relevancy': 1.0000, 'faithfulness': 0.0000, 'context_precision': 0.0000, 'context_recall': 0.0000}\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "context = ['Vienna is the capital of Austria']\n",
    "question = questions[-1]\n",
    "answer = generate_answer(question, context)\n",
    "\n",
    "rows.append(\n",
    "    {\n",
    "        \"user_input\": question,\n",
    "        \"retrieved_contexts\": context,\n",
    "        \"response\": \"Mike's favorite color is pink!\",\n",
    "        \"reference\": ground_truths[-1]\n",
    "    }\n",
    ")\n",
    "\n",
    "evaluation_dataset = Dataset.from_list(rows)\n",
    "\n",
    "scores = evaluate(\n",
    "    evaluation_dataset,\n",
    "    metrics=[\n",
    "        answer_correctness,\n",
    "        answer_relevancy,\n",
    "        faithfulness,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(rows)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00463c6-aa93-4346-8cc2-b4e0688272fa",
   "metadata": {},
   "source": [
    "## Ollama Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ad58b2c-9c45-4426-82c3-b138d4df0ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894ba164-c756-4240-91e3-19b41bd1d5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"qwen3:4b\", temperature=0)\n",
    "ragas_llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "emb = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "ragas_emb = LangchainEmbeddingsWrapper(emb)\n",
    "\n",
    "scores = evaluate(\n",
    "    evaluation_dataset,\n",
    "    metrics=[answer_correctness, answer_relevancy, faithfulness,\n",
    "             context_precision, context_recall],\n",
    "    llm=ragas_llm,\n",
    "    embeddings=ragas_emb,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234b952c-4fe5-4eb7-a844-b8a1ec306ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a876a35d-6c88-4352-a55d-0bcb50d71044",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
